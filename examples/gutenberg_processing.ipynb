{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import polars as pl\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Use the specified data directory path\n",
    "data_dir = \"/Users/tommasofurlanello/Documents/Dev/MarketInference/data\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "def download_gutenberg_dataset():\n",
    "    \"\"\"\n",
    "    Download the Gutenberg English dataset from Hugging Face and save to /data folder\n",
    "    \"\"\"\n",
    "    print(\"Downloading Gutenberg English dataset...\")\n",
    "    \n",
    "    # Method 1: Using the datasets library\n",
    "    try:\n",
    "        dataset = load_dataset(\"sedthh/gutenberg_english\")\n",
    "        # Save each split to parquet files for faster loading\n",
    "        for split in dataset:\n",
    "            output_path = os.path.join(data_dir, f\"gutenberg_{split}.parquet\")\n",
    "            print(f\"Saving {split} split to {output_path}\")\n",
    "            dataset[split].to_parquet(output_path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error using datasets library: {e}\")\n",
    "        print(\"Trying alternative download method...\")\n",
    "    \n",
    "    # Method 2: Manual download if datasets library fails\n",
    "    try:\n",
    "        # Get dataset info\n",
    "        info_url = \"https://huggingface.co/datasets/sedthh/gutenberg_english/resolve/main/dataset_infos.json\"\n",
    "        response = requests.get(info_url)\n",
    "        response.raise_for_status()\n",
    "        info = json.loads(response.text)\n",
    "        \n",
    "        splits = [\"train\", \"test\", \"validation\"]\n",
    "        for split in splits:\n",
    "            # Download each split\n",
    "            split_url = f\"https://huggingface.co/datasets/sedthh/gutenberg_english/resolve/main/{split}-00000-of-00001.parquet\"\n",
    "            output_path = os.path.join(data_dir, f\"gutenberg_{split}.parquet\")\n",
    "            \n",
    "            print(f\"Downloading {split} split from {split_url}\")\n",
    "            response = requests.get(split_url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            with open(output_path, \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Saved to {output_path}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error in alternative download method: {e}\")\n",
    "        return False\n",
    "\n",
    "def load_as_polars():\n",
    "    \"\"\"\n",
    "    Load all splits of the Gutenberg dataset as a single Polars DataFrame\n",
    "    \"\"\"\n",
    "    splits = [\"train\", \"test\", \"validation\"]\n",
    "    dataframes = []\n",
    "    \n",
    "    for split in splits:\n",
    "        file_path = os.path.join(data_dir, f\"gutenberg_{split}.parquet\")\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"Loading {file_path}...\")\n",
    "            df = pl.read_parquet(file_path)\n",
    "            # Add a column to identify the split\n",
    "            df = df.with_columns(pl.lit(split).alias(\"split\"))\n",
    "            dataframes.append(df)\n",
    "        else:\n",
    "            print(f\"Warning: {file_path} not found\")\n",
    "    \n",
    "    if not dataframes:\n",
    "        raise FileNotFoundError(f\"No dataset files found in {data_dir} directory\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    combined_df = pl.concat(dataframes)\n",
    "    print(f\"Combined DataFrame shape: {combined_df.shape}\")\n",
    "    print(f\"Combined DataFrame schema:\\n{combined_df.schema}\")\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Gutenberg English dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 37/37 [03:56<00:00,  6.39s/files]\n",
      "Generating train split: 100%|██████████| 48284/48284 [00:16<00:00, 3006.82 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving train split to /Users/tommasofurlanello/Documents/Dev/MarketInference/data/gutenberg_train.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 49/49 [00:41<00:00,  1.19ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_gutenberg_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /Users/tommasofurlanello/Documents/Dev/MarketInference/data/gutenberg_train.parquet...\n",
      "Warning: /Users/tommasofurlanello/Documents/Dev/MarketInference/data/gutenberg_test.parquet not found\n",
      "Warning: /Users/tommasofurlanello/Documents/Dev/MarketInference/data/gutenberg_validation.parquet not found\n",
      "Combined DataFrame shape: (48284, 4)\n",
      "Combined DataFrame schema:\n",
      "Schema({'TEXT': String, 'SOURCE': String, 'METADATA': String, 'split': String})\n"
     ]
    }
   ],
   "source": [
    "df = load_as_polars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnested_df = df.with_columns(pl.col(\"METADATA\").str.json_decode()).unnest(\"METADATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Historical fiction; War stories; United States -- History -- Civil War, 1861-1865 -- Fiction; Virginia -- History -- Civil War, 1861-1865 -- Fiction; Chancellorsville, Battle of, Chancellorsville, Va., 1863 -- Fiction'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unnested_df[\"subjects\"][52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels = unnested_df.filter(pl.col(\"subjects\").str.contains(\"fiction\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels.write_parquet(\"/Users/tommasofurlanello/Documents/Dev/MarketInference/data/gutenberg_en_novels.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CHAPTER I.Down the Rabbit-Hole\\r\\n\\r\\n\\r\\n CHAPTER II.The Pool of Tears\\r\\n\\r\\n\\r\\n CHAPTER III.A Caucus-Race and a Long Tale\\r\\n\\r\\n\\r\\n CHAPTER IV.The Rabbit Sends in a Little Bill\\r\\n\\r\\n\\r\\n CHAPTER V.Advice from a Caterpillar\\r\\n\\r\\n\\r\\n CHAPTER VI.Pig and Pepper\\r\\n\\r\\n\\r\\n CHAPTER VII.A Mad Tea-Party\\r\\n\\r\\n\\r\\n CHAPTER VIII.The Queen’s Croquet-Ground\\r\\n\\r\\n\\r\\n CHAPTER IX.The Mock Turtle’s Story\\r\\n\\r\\n\\r\\n CHAPTER X.The Lobster Quadrille\\r\\n\\r\\n\\r\\n CHAPTER XI.Who Stole the Tarts?\\r\\n\\r\\n\\r\\n CHAPTER XII.Alice’s Evidence\\r\\n\\r\\n\\r\\nCHAPTER I.\\r\\n\\r\\nDown the Rabbit-Hole\\r\\n\\r\\n\\r\\nAlice was beginning to get very tired of sitting by her sister on the bank, and\\r\\n\\r\\nof having nothing to do: once or twice she had peeped into the book her sister\\r\\n\\r\\nwas reading, but it had no pictures or conversations in it, “and what is\\r\\n\\r\\nthe use of a book,” thought Alice “without pictures or\\r\\n\\r\\nconversations?”\\r\\n\\r\\n\\r\\nSo she was considering in her own mind (as well as she could, for the hot day\\r\\n\\r\\nmade her feel very sleepy and stupid), whether the pleasure of making a\\r\\n\\r\\ndaisy-chain would be worth the trouble of getting up and picking the daisies,\\r\\n\\r\\nwhen suddenly a White Rabbit with pink eyes ran close by her.\\r\\n\\r\\n\\r\\nThere was nothing so very remarkable in that; nor did Alice think it so\\r\\n\\r\\nvery much out of the way to hear the Rabbit say to itself, “Oh\\r\\n\\r\\ndear! Oh dear! I shall be late!” (when she thought it over afterwards, it\\r\\n\\r\\noccurred to her that she ought to have wondered at this, but at the time it all\\r\\n\\r\\nseemed quite natural); but when the Rabbit actually took a watch out of its\\r\\n\\r\\nwaistcoat-pocket, and looked at it, and then hurried on, Alice started to\\r\\n\\r\\nher feet, for it flashed across her mind that she had never before seen a\\r\\n\\r\\nrabbit with either a waistcoat-pocket, or a watch to take out of it, and\\r\\n\\r\\nburning with curiosity, she ran across the field after it, and fortunately was\\r\\n\\r\\njust in time to see it pop down a large rabbit-hole under the hedge.\\r\\n\\r\\n\\r\\nIn another moment down went Alice after it, never once considering how in the\\r\\n\\r\\nworld she was to get out again.\\r\\n\\r\\n\\r\\nThe rabbit-hole went straight on like a tunnel for some way, and then dipped\\r\\n\\r\\nsuddenly down, so suddenly that Alice had not a moment to think about stopping\\r\\n\\r\\nherself before she found herself falling down a very deep well.\\r\\n\\r\\n\\r\\nEither the well was very deep, or she fell very slowly, for she had plenty of\\r\\n\\r\\ntime as she went down to look about her and to won'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novels[\"TEXT\"][0][114:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
